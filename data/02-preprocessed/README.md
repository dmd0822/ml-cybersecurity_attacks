# `data/02-preprocessed/`

This folder stores **cleaned and standardized datasets** derived from `data/01-raw/`.

Preprocessing typically includes parsing, normalization, deduplication, basic validation, and joining reference data.

## What belongs here

- Clean tables ready for feature engineering
- Standardized schemas and normalized units
- Train/validation/test splits (if you store them as data artifacts)

## Conventions (recommended)

- Keep preprocessing outputs deterministic and reproducible.
- Track schema expectations (columns, types, nullability).
- Avoid leaking target information into preprocessing outputs.

## Example structure

```text
data/02-preprocessed/
  dataset_name/
    v1/
```

## Current prepared dataset format

Prepared datasets written by `python entrypoints/prepare_dataset.py` use a single folder per run, e.g.:

```text
data/02-preprocessed/
  cybersecurity_attacks_v1_2025-12-29/
    cleaned.parquet   # when Parquet support is available
    cleaned.csv       # fallback
    split.csv
    attack_type_classes.json
    baseline_feature_config.json
    feature_audit.csv
    manifest.json
```

Notes:

- The split is a shared stratified `train`/`val`/`test` assignment.
- Training/inference pipelines and notebooks default to using the *latest* prepared dataset folder.

## How This Fits

- Upstream: raw inputs in [`data/01-raw/`](../01-raw/)
- Downstream: feature artifacts in [`data/03-features/`](../03-features/)
- Generated by pipeline code in [`src/pipelines/`](../../src/pipelines/)

