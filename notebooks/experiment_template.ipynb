{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f1f8ff",
   "metadata": {},
   "source": [
    "# Experiment template (baseline feature config)\n",
    "\n",
    "This notebook is a minimal template for model experiments.\n",
    "\n",
    "It loads:\n",
    "- `cleaned.parquet` (or `cleaned.csv`)\n",
    "- `split.csv`\n",
    "- `config/baseline_feature_config.json`\n",
    "\n",
    "Then applies baseline drops/transforms before fitting a one-hot + numeric preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03099a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prepared dataset: C:\\repos\\ml-cybersecurity_attacks\\data\\02-preprocessed\\cybersecurity_attacks_v1_2025-12-29\n",
      "Using baseline config:  C:\\repos\\ml-cybersecurity_attacks\\config\\baseline_feature_config.json\n",
      "Shapes:\n",
      "  df:     (40000, 26)\n",
      "  X_full: (40000, 25)\n",
      "  y_full: (40000,)\n",
      "Split shapes:\n",
      "  train: (28000, 25), labels: (28000,)\n",
      "  val:   (6000, 25), labels: (6000,)\n",
      "  test:  (6000, 25), labels: (6000,)\n",
      "\n",
      "Label distribution by split:\n",
      "\n",
      "train:\n",
      "Attack Type\n",
      "DDoS         9400\n",
      "Malware      9315\n",
      "Intrusion    9285\n",
      "Name: count, dtype: int64\n",
      "\n",
      "val:\n",
      "Attack Type\n",
      "DDoS         2014\n",
      "Malware      1996\n",
      "Intrusion    1990\n",
      "Name: count, dtype: int64\n",
      "\n",
      "test:\n",
      "Attack Type\n",
      "DDoS         2014\n",
      "Malware      1996\n",
      "Intrusion    1990\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Paths ---\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]  # notebooks/ -> repo root\n",
    "if (REPO_ROOT / \"src\").exists():\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "PREPROCESSED_ROOT = REPO_ROOT / \"data\" / \"02-preprocessed\"\n",
    "BASELINE_CONFIG_JSON = REPO_ROOT / \"config\" / \"baseline_feature_config.json\"\n",
    "\n",
    "# Locate latest prepared dataset folder\n",
    "prepared_dirs = sorted(\n",
    "    [p for p in PREPROCESSED_ROOT.iterdir() if p.is_dir()],\n",
    "    key=lambda p: p.name,\n",
    ")\n",
    "if not prepared_dirs:\n",
    "    raise FileNotFoundError(f\"No prepared datasets found under: {PREPROCESSED_ROOT}\")\n",
    "\n",
    "DATASET_DIR = prepared_dirs[-1]\n",
    "\n",
    "cleaned_parquet = DATASET_DIR / \"cleaned.parquet\"\n",
    "cleaned_csv = DATASET_DIR / \"cleaned.csv\"\n",
    "split_csv = DATASET_DIR / \"split.csv\"\n",
    "\n",
    "print(f\"Using prepared dataset: {DATASET_DIR}\")\n",
    "print(f\"Using baseline config:  {BASELINE_CONFIG_JSON}\")\n",
    "\n",
    "# Load cleaned dataset\n",
    "if cleaned_parquet.exists():\n",
    "    df = pd.read_parquet(cleaned_parquet)\n",
    "elif cleaned_csv.exists():\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Expected cleaned.parquet or cleaned.csv\")\n",
    "\n",
    "splits = pd.read_csv(split_csv)\n",
    "\n",
    "# Load baseline feature config and apply it\n",
    "from src.pipelines.features import apply_baseline_feature_config, load_baseline_feature_config\n",
    "\n",
    "cfg = load_baseline_feature_config(BASELINE_CONFIG_JSON)\n",
    "\n",
    "# Sanity checks\n",
    "required_cols = {cfg.row_id_col, cfg.target_col}\n",
    "missing_required = required_cols - set(df.columns)\n",
    "if missing_required:\n",
    "    raise KeyError(f\"Missing required columns in cleaned data: {sorted(missing_required)}\")\n",
    "\n",
    "X_full = apply_baseline_feature_config(df, cfg)\n",
    "y_full = df[cfg.target_col].astype(str)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(f\"  df:     {df.shape}\")\n",
    "print(f\"  X_full: {X_full.shape}\")\n",
    "print(f\"  y_full: {y_full.shape}\")\n",
    "\n",
    "# Join split info\n",
    "df_split = df[[cfg.row_id_col]].merge(splits[[cfg.row_id_col, \"split\"]], on=cfg.row_id_col, how=\"left\")\n",
    "if df_split[\"split\"].isna().any():\n",
    "    raise ValueError(\"Some rows are missing split assignments (split.csv join failed)\")\n",
    "\n",
    "mask_train = df_split[\"split\"].eq(\"train\")\n",
    "mask_val = df_split[\"split\"].eq(\"val\")\n",
    "mask_test = df_split[\"split\"].eq(\"test\")\n",
    "\n",
    "X_train, y_train = X_full.loc[mask_train].reset_index(drop=True), y_full.loc[mask_train].reset_index(drop=True)\n",
    "X_val, y_val = X_full.loc[mask_val].reset_index(drop=True), y_full.loc[mask_val].reset_index(drop=True)\n",
    "X_test, y_test = X_full.loc[mask_test].reset_index(drop=True), y_full.loc[mask_test].reset_index(drop=True)\n",
    "\n",
    "print(\"Split shapes:\")\n",
    "print(f\"  train: {X_train.shape}, labels: {y_train.shape}\")\n",
    "print(f\"  val:   {X_val.shape}, labels: {y_val.shape}\")\n",
    "print(f\"  test:  {X_test.shape}, labels: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nLabel distribution by split:\")\n",
    "for name, y in [(\"train\", y_train), (\"val\", y_val), (\"test\", y_test)]:\n",
    "    vc = y.value_counts(dropna=False)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(vc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83e7377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols: 11\n",
      "Numeric cols:     14\n"
     ]
    }
   ],
   "source": [
    "# --- Build preprocessing for one-hot baseline ---\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\" or str(X_train[c].dtype).startswith(\"string\")]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\n",
    "                        \"onehot\",\n",
    "                        OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            cat_cols,\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]),\n",
    "            num_cols,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "print(f\"Categorical cols: {len(cat_cols)}\")\n",
    "print(f\"Numeric cols:     {len(num_cols)}\")\n",
    "\n",
    "# Placeholder: plug in a model below.\n",
    "# Example:\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression(max_iter=1000, class_weight='balanced', n_jobs=None)\n",
    "# clf = Pipeline(steps=[('preprocess', preprocess), ('model', model)])\n",
    "# clf.fit(X_train, y_train)\n",
    "# print('val accuracy:', clf.score(X_val, y_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
