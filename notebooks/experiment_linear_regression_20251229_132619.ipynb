{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f30cdab",
   "metadata": {},
   "source": [
    "# Experiment: Linear Regression (OVR argmax)\n",
    "\n",
    "Task: **classification**\n",
    "\n",
    "This notebook maps linear regression to classification via one-vs-rest regression and argmax. Prefer logistic regression for a probabilistic linear baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]  # notebooks/ -> repo root\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "PREPROCESSED_ROOT = REPO_ROOT / 'data' / '02-preprocessed'\n",
    "BASELINE_CONFIG_JSON = REPO_ROOT / 'config' / 'baseline_feature_config.json'\n",
    "\n",
    "prepared_dirs = sorted([p for p in PREPROCESSED_ROOT.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "if not prepared_dirs:\n",
    "    raise FileNotFoundError(f'No prepared datasets found under: {PREPROCESSED_ROOT}')\n",
    "DATASET_DIR = prepared_dirs[-1]\n",
    "\n",
    "cleaned_parquet = DATASET_DIR / 'cleaned.parquet'\n",
    "cleaned_csv = DATASET_DIR / 'cleaned.csv'\n",
    "split_csv = DATASET_DIR / 'split.csv'\n",
    "\n",
    "print(f'Using prepared dataset: {DATASET_DIR}')\n",
    "print(f'Using baseline config:  {BASELINE_CONFIG_JSON}')\n",
    "\n",
    "if cleaned_parquet.exists():\n",
    "    df = pd.read_parquet(cleaned_parquet)\n",
    "elif cleaned_csv.exists():\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "else:\n",
    "    raise FileNotFoundError('Expected cleaned.parquet or cleaned.csv')\n",
    "\n",
    "splits = pd.read_csv(split_csv)\n",
    "\n",
    "from src.pipelines.features import apply_baseline_feature_config, load_baseline_feature_config\n",
    "cfg = load_baseline_feature_config(BASELINE_CONFIG_JSON)\n",
    "\n",
    "required_cols = {cfg.row_id_col, cfg.target_col}\n",
    "missing_required = required_cols - set(df.columns)\n",
    "if missing_required:\n",
    "    raise KeyError(f'Missing required columns in cleaned data: {sorted(missing_required)}')\n",
    "\n",
    "X_full = apply_baseline_feature_config(df, cfg)\n",
    "y_full = df[cfg.target_col].astype(str)\n",
    "\n",
    "df_split = df[[cfg.row_id_col]].merge(splits[[cfg.row_id_col, 'split']], on=cfg.row_id_col, how='left')\n",
    "if df_split['split'].isna().any():\n",
    "    raise ValueError('Some rows are missing split assignments (split.csv join failed)')\n",
    "\n",
    "mask_train = df_split['split'].eq('train')\n",
    "mask_val = df_split['split'].eq('val')\n",
    "mask_test = df_split['split'].eq('test')\n",
    "\n",
    "X_train, y_train = X_full.loc[mask_train].reset_index(drop=True), y_full.loc[mask_train].reset_index(drop=True)\n",
    "X_val, y_val = X_full.loc[mask_val].reset_index(drop=True), y_full.loc[mask_val].reset_index(drop=True)\n",
    "X_test, y_test = X_full.loc[mask_test].reset_index(drop=True), y_full.loc[mask_test].reset_index(drop=True)\n",
    "\n",
    "print('Split sizes:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ccf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing: impute + one-hot for categoricals; impute (+ optional scale) for numeric\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == 'object' or str(X_train[c].dtype).startswith('string')]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "num_steps = [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "]\n",
    "num_steps = [s for s in num_steps if not (isinstance(s, str) or s[0].startswith('#'))]\n",
    "num_pipe = Pipeline(steps=num_steps)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[('cat', cat_pipe, cat_cols), ('num', num_pipe, num_cols)],\n",
    "    remainder='drop',\n",
    ")\n",
    "\n",
    "print(f'Categorical cols: {len(cat_cols)}')\n",
    "print(f'Numeric cols:     {len(num_cols)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition + training\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "model = OneVsRestClassifier(LinearRegression())\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline(steps=[('preprocess', preprocess), ('model', model)])\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "def eval_split(name: str, y_true, y_pred) -> None:\n",
    "    print(f'\\n== {name} ==')\n",
    "    print('macro_f1:', f1_score(y_true, y_pred, average='macro'))\n",
    "    print('weighted_f1:', f1_score(y_true, y_pred, average='weighted'))\n",
    "    print('balanced_acc:', balanced_accuracy_score(y_true, y_pred))\n",
    "    print('confusion_matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "val_pred = clf.predict(X_val)\n",
    "test_pred = clf.predict(X_test)\n",
    "\n",
    "eval_split('val', y_val, val_pred)\n",
    "eval_split('test', y_test, test_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}