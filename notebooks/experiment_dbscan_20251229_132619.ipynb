{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e70abb",
   "metadata": {},
   "source": [
    "# Experiment: DBSCAN\n",
    "\n",
    "Task: **clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f91341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]  # notebooks/ -> repo root\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "PREPROCESSED_ROOT = REPO_ROOT / 'data' / '02-preprocessed'\n",
    "BASELINE_CONFIG_JSON = REPO_ROOT / 'config' / 'baseline_feature_config.json'\n",
    "\n",
    "prepared_dirs = sorted([p for p in PREPROCESSED_ROOT.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "if not prepared_dirs:\n",
    "    raise FileNotFoundError(f'No prepared datasets found under: {PREPROCESSED_ROOT}')\n",
    "DATASET_DIR = prepared_dirs[-1]\n",
    "\n",
    "cleaned_parquet = DATASET_DIR / 'cleaned.parquet'\n",
    "cleaned_csv = DATASET_DIR / 'cleaned.csv'\n",
    "split_csv = DATASET_DIR / 'split.csv'\n",
    "\n",
    "print(f'Using prepared dataset: {DATASET_DIR}')\n",
    "print(f'Using baseline config:  {BASELINE_CONFIG_JSON}')\n",
    "\n",
    "if cleaned_parquet.exists():\n",
    "    df = pd.read_parquet(cleaned_parquet)\n",
    "elif cleaned_csv.exists():\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "else:\n",
    "    raise FileNotFoundError('Expected cleaned.parquet or cleaned.csv')\n",
    "\n",
    "splits = pd.read_csv(split_csv)\n",
    "\n",
    "from src.pipelines.features import apply_baseline_feature_config, load_baseline_feature_config\n",
    "cfg = load_baseline_feature_config(BASELINE_CONFIG_JSON)\n",
    "\n",
    "required_cols = {cfg.row_id_col, cfg.target_col}\n",
    "missing_required = required_cols - set(df.columns)\n",
    "if missing_required:\n",
    "    raise KeyError(f'Missing required columns in cleaned data: {sorted(missing_required)}')\n",
    "\n",
    "X_full = apply_baseline_feature_config(df, cfg)\n",
    "y_full = df[cfg.target_col].astype(str)\n",
    "\n",
    "df_split = df[[cfg.row_id_col]].merge(splits[[cfg.row_id_col, 'split']], on=cfg.row_id_col, how='left')\n",
    "if df_split['split'].isna().any():\n",
    "    raise ValueError('Some rows are missing split assignments (split.csv join failed)')\n",
    "\n",
    "mask_train = df_split['split'].eq('train')\n",
    "mask_val = df_split['split'].eq('val')\n",
    "mask_test = df_split['split'].eq('test')\n",
    "\n",
    "X_train, y_train = X_full.loc[mask_train].reset_index(drop=True), y_full.loc[mask_train].reset_index(drop=True)\n",
    "X_val, y_val = X_full.loc[mask_val].reset_index(drop=True), y_full.loc[mask_val].reset_index(drop=True)\n",
    "X_test, y_test = X_full.loc[mask_test].reset_index(drop=True), y_full.loc[mask_test].reset_index(drop=True)\n",
    "\n",
    "print('Split sizes:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing: impute + one-hot for categoricals; impute (+ optional scale) for numeric\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == 'object' or str(X_train[c].dtype).startswith('string')]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "num_steps = [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "]\n",
    "num_steps = [s for s in num_steps if not (isinstance(s, str) or s[0].startswith('#'))]\n",
    "num_pipe = Pipeline(steps=num_steps)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[('cat', cat_pipe, cat_cols), ('num', num_pipe, num_cols)],\n",
    "    remainder='drop',\n",
    ")\n",
    "\n",
    "print(f'Categorical cols: {len(cat_cols)}')\n",
    "print(f'Numeric cols:     {len(num_cols)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition + clustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "model = DBSCAN(eps=0.5, min_samples=10)\n",
    "\n",
    "# For clustering, fit on train only (no leakage), then evaluate cluster quality vs labels.\n",
    "X_train_t = preprocess.fit_transform(X_train)\n",
    "train_labels = model.fit_predict(X_train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def eval_clusters(name: str, y_true, labels) -> None:\n",
    "    print(f'\\n== {name} ==')\n",
    "    print('ARI:', adjusted_rand_score(y_true, labels))\n",
    "    print('NMI:', normalized_mutual_info_score(y_true, labels))\n",
    "\n",
    "eval_clusters('train', y_train, train_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}