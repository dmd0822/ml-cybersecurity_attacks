{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ff7e17",
   "metadata": {},
   "source": [
    "# Experiment: Gradient Boosting Regression\n",
    "\n",
    "Task: **regression**\n",
    "\n",
    "Target: `Anomaly Scores`\n",
    "\n",
    "Shared split: `split_time_70_15_15.csv` (time-aware, timestamp-grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Resolve repo root robustly so this works from notebooks/ or notebooks/anomaly_scores/.\n",
    "cwd = Path.cwd().resolve()\n",
    "candidates = [cwd] + list(cwd.parents)\n",
    "REPO_ROOT = next((p for p in candidates if (p / 'src').exists() and (p / 'data').exists()), None)\n",
    "if REPO_ROOT is None:\n",
    "    raise FileNotFoundError('Could not locate repo root (expected src/ and data/).')\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "PREPROCESSED_ROOT = REPO_ROOT / 'data' / '02-preprocessed'\n",
    "BASELINE_CONFIG_JSON = REPO_ROOT / 'config' / 'baseline_feature_config.json'\n",
    "VIZ_CONFIG_JSON = REPO_ROOT / 'config' / 'visualization.json'\n",
    "\n",
    "prepared_dirs = sorted([p for p in PREPROCESSED_ROOT.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "if not prepared_dirs:\n",
    "    raise FileNotFoundError(f'No prepared datasets found under: {PREPROCESSED_ROOT}')\n",
    "DATASET_DIR = prepared_dirs[-1]\n",
    "\n",
    "cleaned_parquet = DATASET_DIR / 'cleaned.parquet'\n",
    "cleaned_csv = DATASET_DIR / 'cleaned.csv'\n",
    "split_time_csv = DATASET_DIR / 'split_time_70_15_15.csv'\n",
    "\n",
    "print(f'Using prepared dataset: {DATASET_DIR}')\n",
    "print(f'Using baseline config:  {BASELINE_CONFIG_JSON}')\n",
    "print(f'Using time split:      {split_time_csv.name}')\n",
    "\n",
    "if cleaned_parquet.exists():\n",
    "    df = pd.read_parquet(cleaned_parquet)\n",
    "elif cleaned_csv.exists():\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "else:\n",
    "    raise FileNotFoundError('Expected cleaned.parquet or cleaned.csv')\n",
    "\n",
    "if not split_time_csv.exists():\n",
    "    raise FileNotFoundError(f'Missing time-aware split artifact: {split_time_csv}')\n",
    "splits = pd.read_csv(split_time_csv)\n",
    "\n",
    "from src.pipelines.features import (\n",
    "    BaselineFeatureConfig,\n",
    "    apply_baseline_feature_config,\n",
    "    load_baseline_feature_config,\n",
    ")\n",
    "\n",
    "cfg0 = load_baseline_feature_config(BASELINE_CONFIG_JSON)\n",
    "# Override the target to Anomaly Scores (regression) and drop Attack Type to reduce leakage.\n",
    "cfg = BaselineFeatureConfig(\n",
    "    target_col='Anomaly Scores',\n",
    "    row_id_col=cfg0.row_id_col,\n",
    "    drop_cols=sorted(set(cfg0.drop_cols + ['Attack Type'])),\n",
    "    timestamp_cols=cfg0.timestamp_cols,\n",
    "    port_cols=cfg0.port_cols,\n",
    ")\n",
    "\n",
    "required_cols = {cfg.row_id_col, cfg.target_col, 'Timestamp'}\n",
    "missing_required = required_cols - set(df.columns)\n",
    "if missing_required:\n",
    "    raise KeyError(f'Missing required columns in cleaned data: {sorted(missing_required)}')\n",
    "\n",
    "# Features and regression target\n",
    "X_full = apply_baseline_feature_config(df, cfg)\n",
    "y_full = pd.to_numeric(df[cfg.target_col], errors='coerce')\n",
    "\n",
    "# Split join by row_id\n",
    "df_split = df[[cfg.row_id_col]].merge(splits[[cfg.row_id_col, 'split']], on=cfg.row_id_col, how='left')\n",
    "if df_split['split'].isna().any():\n",
    "    raise ValueError('Some rows are missing split assignments (time split join failed)')\n",
    "\n",
    "mask_train = df_split['split'].eq('train')\n",
    "mask_val = df_split['split'].eq('val')\n",
    "mask_test = df_split['split'].eq('test')\n",
    "\n",
    "X_train, y_train = X_full.loc[mask_train].reset_index(drop=True), y_full.loc[mask_train].reset_index(drop=True)\n",
    "X_val, y_val = X_full.loc[mask_val].reset_index(drop=True), y_full.loc[mask_val].reset_index(drop=True)\n",
    "X_test, y_test = X_full.loc[mask_test].reset_index(drop=True), y_full.loc[mask_test].reset_index(drop=True)\n",
    "\n",
    "# Keep timestamps for test rows for time-based charts\n",
    "ts_full = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True)\n",
    "ts_test = ts_full.loc[mask_test].reset_index(drop=True)\n",
    "\n",
    "print('Split sizes:', X_train.shape, X_val.shape, X_test.shape)\n",
    "print('Target summary (train):', y_train.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc682c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing: impute + one-hot for categoricals; impute (+ optional scale) for numeric\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == 'object' or str(X_train[c].dtype).startswith('string')]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "num_steps = [\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # ('scaler', StandardScaler()),\n",
    "]\n",
    "num_steps = [s for s in num_steps if not (isinstance(s, str) or s[0].startswith('#'))]\n",
    "num_pipe = Pipeline(steps=num_steps)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[('cat', cat_pipe, cat_cols), ('num', num_pipe, num_cols)],\n",
    "    remainder='drop',\n",
    ")\n",
    "\n",
    "print(f'Categorical cols: {len(cat_cols)}')\n",
    "print(f'Numeric cols:     {len(num_cols)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition + training\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=SEED)\n",
    "\n",
    "reg = Pipeline(steps=[('preprocess', preprocess), ('model', model)])\n",
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2463427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def eval_split(name: str, y_true, y_pred) -> dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'\\n== {name} ==')\n",
    "    print(f'MAE:  {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'R2:   {r2:.4f}')\n",
    "    return {'mae': float(mae), 'rmse': float(rmse), 'r2': float(r2)}\n",
    "\n",
    "val_pred = reg.predict(X_val)\n",
    "test_pred = reg.predict(X_test)\n",
    "\n",
    "val_metrics = eval_split('val', y_val, val_pred)\n",
    "test_metrics = eval_split('test', y_test, test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charts (uses config/visualization.json color_palette via COLOR_PALLETE)\n",
    "\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "def load_color_palette(path: Path) -> list[str]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        payload = json.load(f)\n",
    "    palette = payload.get('visualization', {}).get('color_palette', [])\n",
    "    if not isinstance(palette, list) or not palette:\n",
    "        raise ValueError(f\"Missing/invalid visualization.color_palette in {path}\")\n",
    "    return [str(c) for c in palette]\n",
    "\n",
    "COLOR_PALLETE = load_color_palette(VIZ_CONFIG_JSON)\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=COLOR_PALLETE)\n",
    "print(f'Loaded COLOR_PALLETE ({len(COLOR_PALLETE)} colors) from {VIZ_CONFIG_JSON.name}')\n",
    "\n",
    "# 1) Target distribution (train)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(y_train, bins=50, color=COLOR_PALLETE[0], alpha=0.9)\n",
    "ax.set_title('Anomaly Scores distribution (train)')\n",
    "ax.set_xlabel('Anomaly Scores')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Predicted vs actual (test)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(y_test, test_pred, s=12, alpha=0.6, color=COLOR_PALLETE[1])\n",
    "mn = float(min(y_test.min(), test_pred.min()))\n",
    "mx = float(max(y_test.max(), test_pred.max()))\n",
    "ax.plot([mn, mx], [mn, mx], linestyle='--', color=COLOR_PALLETE[-1], linewidth=1)\n",
    "ax.set_title('Predicted vs actual (test)')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Residual histogram (test)\n",
    "residuals = (y_test - test_pred)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(residuals, bins=50, color=COLOR_PALLETE[2], alpha=0.9)\n",
    "ax.axvline(0.0, color=COLOR_PALLETE[-1], linestyle='--', linewidth=1)\n",
    "ax.set_title('Residuals (test): actual - predicted')\n",
    "ax.set_xlabel('Residual')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Actual vs predicted over time (test; chronological)\n",
    "df_time = pd.DataFrame({'Timestamp': ts_test, 'actual': y_test, 'pred': test_pred}).dropna()\n",
    "df_time = df_time.sort_values('Timestamp').reset_index(drop=True)\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(df_time['Timestamp'], df_time['actual'], color=COLOR_PALLETE[3], linewidth=1, label='actual')\n",
    "ax.plot(df_time['Timestamp'], df_time['pred'], color=COLOR_PALLETE[4], linewidth=1, label='pred')\n",
    "ax.set_title('Anomaly Scores over time (test)')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Anomaly Scores')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
