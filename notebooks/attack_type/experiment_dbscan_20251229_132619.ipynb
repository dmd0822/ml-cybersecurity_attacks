{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e70abb",
   "metadata": {},
   "source": [
    "# Experiment: DBSCAN\n",
    "\n",
    "Task: **clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f91341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prepared dataset: C:\\repos\\ml-cybersecurity_attacks\\data\\02-preprocessed\\cybersecurity_attacks_v1_2025-12-29\n",
      "Using baseline config:  C:\\repos\\ml-cybersecurity_attacks\\config\\baseline_feature_config.json\n",
      "Split sizes: (28000, 25) (6000, 25) (6000, 25)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"Find the repo root by walking upward until key markers are found.\"\"\"\n",
    "    start = start.resolve()\n",
    "    for candidate in (start, *start.parents):\n",
    "        if (candidate / 'requirements.txt').exists() and (candidate / 'src').is_dir():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f'Could not locate repo root from {start}. Expected to find requirements.txt and src/.'\n",
    "    )\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "PREPROCESSED_ROOT = REPO_ROOT / 'data' / '02-preprocessed'\n",
    "BASELINE_CONFIG_JSON = REPO_ROOT / 'config' / 'baseline_feature_config.json'\n",
    "\n",
    "if not PREPROCESSED_ROOT.exists():\n",
    "    raise FileNotFoundError(f'Preprocessed root not found: {PREPROCESSED_ROOT}')\n",
    "\n",
    "prepared_dirs = sorted([p for p in PREPROCESSED_ROOT.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "if not prepared_dirs:\n",
    "    raise FileNotFoundError(f'No prepared datasets found under: {PREPROCESSED_ROOT}')\n",
    "DATASET_DIR = prepared_dirs[-1]\n",
    "\n",
    "cleaned_parquet = DATASET_DIR / 'cleaned.parquet'\n",
    "cleaned_csv = DATASET_DIR / 'cleaned.csv'\n",
    "split_csv = DATASET_DIR / 'split.csv'\n",
    "\n",
    "print(f'Using prepared dataset: {DATASET_DIR}')\n",
    "print(f'Using baseline config:  {BASELINE_CONFIG_JSON}')\n",
    "\n",
    "if cleaned_parquet.exists():\n",
    "    df = pd.read_parquet(cleaned_parquet)\n",
    "elif cleaned_csv.exists():\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "else:\n",
    "    raise FileNotFoundError('Expected cleaned.parquet or cleaned.csv')\n",
    "\n",
    "if not split_csv.exists():\n",
    "    raise FileNotFoundError(f'Expected split.csv at: {split_csv}')\n",
    "splits = pd.read_csv(split_csv)\n",
    "\n",
    "from src.pipelines.features import apply_baseline_feature_config, load_baseline_feature_config\n",
    "cfg = load_baseline_feature_config(BASELINE_CONFIG_JSON)\n",
    "\n",
    "required_cols = {cfg.row_id_col, cfg.target_col}\n",
    "missing_required = required_cols - set(df.columns)\n",
    "if missing_required:\n",
    "    raise KeyError(f'Missing required columns in cleaned data: {sorted(missing_required)}')\n",
    "\n",
    "X_full = apply_baseline_feature_config(df, cfg)\n",
    "y_full = df[cfg.target_col].astype(str)\n",
    "\n",
    "df_split = df[[cfg.row_id_col]].merge(splits[[cfg.row_id_col, 'split']], on=cfg.row_id_col, how='left')\n",
    "if df_split['split'].isna().any():\n",
    "    raise ValueError('Some rows are missing split assignments (split.csv join failed)')\n",
    "\n",
    "mask_train = df_split['split'].eq('train')\n",
    "mask_val = df_split['split'].eq('val')\n",
    "mask_test = df_split['split'].eq('test')\n",
    "\n",
    "X_train, y_train = X_full.loc[mask_train].reset_index(drop=True), y_full.loc[mask_train].reset_index(drop=True)\n",
    "X_val, y_val = X_full.loc[mask_val].reset_index(drop=True), y_full.loc[mask_val].reset_index(drop=True)\n",
    "X_test, y_test = X_full.loc[mask_test].reset_index(drop=True), y_full.loc[mask_test].reset_index(drop=True)\n",
    "\n",
    "print('Split sizes:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e276058e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols: 11\n",
      "Numeric cols:     14\n"
     ]
    }
   ],
   "source": [
    "# Build preprocessing: impute + one-hot for categoricals; impute (+ optional scale) for numeric\n",
    "\n",
    "from pandas.api.types import is_bool_dtype, is_numeric_dtype\n",
    "\n",
    "cat_cols = [\n",
    "    c for c in X_train.columns\n",
    "    if (not is_numeric_dtype(X_train[c])) or is_bool_dtype(X_train[c])\n",
    "]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[('cat', cat_pipe, cat_cols), ('num', num_pipe, num_cols)],\n",
    "    remainder='drop',\n",
    ")\n",
    "\n",
    "print(f'Categorical cols: {len(cat_cols)}')\n",
    "print(f'Numeric cols:     {len(num_cols)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12e054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: clusters=0 | noise=28000 / 28000\n",
      "val  : clusters=0 | noise=6000 / 6000\n",
      "test : clusters=0 | noise=6000 / 6000\n"
     ]
    }
   ],
   "source": [
    "# Model definition + clustering (+ \"predict-like\" assignment for val/test)\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model = DBSCAN(eps=0.5, min_samples=10)\n",
    "\n",
    "# Fit on train only (no leakage).\n",
    "X_train_t = preprocess.fit_transform(X_train)\n",
    "train_labels = model.fit_predict(X_train_t)\n",
    "\n",
    "def assign_dbscan_labels(dbscan_model: DBSCAN, X_new: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Assign DBSCAN cluster labels to new points.\n",
    "\n",
    "    scikit-learn's DBSCAN does not implement predict(). This assigns each new point\n",
    "    the label of its nearest *core sample* within eps; otherwise it is labeled -1.\n",
    "    \"\"\"\n",
    "    if not hasattr(dbscan_model, 'components_') or dbscan_model.components_.shape[0] == 0:\n",
    "        return np.full(X_new.shape[0], -1, dtype=int)\n",
    "\n",
    "    core_X = dbscan_model.components_\n",
    "    core_labels = dbscan_model.labels_[dbscan_model.core_sample_indices_]\n",
    "\n",
    "    nn = NearestNeighbors(radius=dbscan_model.eps)\n",
    "    nn.fit(core_X)\n",
    "\n",
    "    neigh_dist, neigh_ind = nn.radius_neighbors(\n",
    "        X_new, return_distance=True, sort_results=True\n",
    "    )\n",
    "\n",
    "    out = np.full(X_new.shape[0], -1, dtype=int)\n",
    "    for i, inds in enumerate(neigh_ind):\n",
    "        if len(inds) == 0:\n",
    "            continue\n",
    "        out[i] = int(core_labels[inds[0]])\n",
    "    return out\n",
    "\n",
    "X_val_t = preprocess.transform(X_val)\n",
    "X_test_t = preprocess.transform(X_test)\n",
    "\n",
    "val_labels = assign_dbscan_labels(model, X_val_t)\n",
    "test_labels = assign_dbscan_labels(model, X_test_t)\n",
    "\n",
    "def summarize_labels(name: str, labels: np.ndarray) -> None:\n",
    "    n_noise = int(np.sum(labels == -1))\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f'{name}: clusters={n_clusters} | noise={n_noise} / {len(labels)}')\n",
    "\n",
    "summarize_labels('train', train_labels)\n",
    "summarize_labels('val  ', val_labels)\n",
    "summarize_labels('test ', test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594a43e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== train ==\n",
      "ARI: 0.0\n",
      "NMI: 0.0\n",
      "\n",
      "== val ==\n",
      "ARI: 0.0\n",
      "NMI: 0.0\n",
      "\n",
      "== test ==\n",
      "ARI: 0.0\n",
      "NMI: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "def eval_clusters(name: str, y_true, labels) -> None:\n",
    "    print(f'\\n== {name} ==')\n",
    "    print('ARI:', adjusted_rand_score(y_true, labels))\n",
    "    print('NMI:', normalized_mutual_info_score(y_true, labels))\n",
    "\n",
    "eval_clusters('train', y_train, train_labels)\n",
    "eval_clusters('val', y_val, val_labels)\n",
    "eval_clusters('test', y_test, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
